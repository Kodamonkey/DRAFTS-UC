<h1 align='center'> DRAFTS </h1>

<div align="center">

âœ¨ **Deep learning-based RAdio Fast Transient Search pipeline** âœ¨

[![TransientSearch](https://img.shields.io/badge/TransientSearch-DRAFTS-da282a)](https://github.com/SukiYume/DRAFTS)
[![GitHub Stars](https://img.shields.io/github/stars/SukiYume/DRAFTS.svg?label=Stars&logo=github)](https://github.com/SukiYume/DRAFTS/stargazers)
[![arXiv](https://img.shields.io/badge/arXiv-2410.03200-b31b1b.svg)](https://arxiv.org/abs/2410.03200)
[![Python](https://img.shields.io/badge/Python-3.6+-blue.svg)](https://www.python.org/)

[Description](#description) â€¢
[Installation](#installation) â€¢
[Usage](#usage) â€¢
[Models](#models) â€¢
[Performance](#performance) â€¢
[Real Data Search](#search-in-real-observation-data) â€¢
[Contributing](#contributing)

</div>

![DRAFTS WorkFlow](./WorkFlow.png)

## Description

**DRAFTS** is a Deep learning-based RAdio Fast Transient Search pipeline designed to address limitations in traditional single-pulse search techniques like Presto and Heimdall.

Traditional methods often face challenges including:

- Complex installation procedures
- Slow execution speeds
- Incomplete search results
- Heavy reliance on manual verification

Our pipeline offers three key components:

1. **CUDA-accelerated de-dispersion** for faster processing
2. **Object detection model** to extract Time of Arrival (TOA) and Dispersion Measure (DM) of FRB signals
3. **Binary classification model** to verify candidate signal authenticity

**Key advantages:**

- Written entirely in Python for easy cross-platform installation
- Achieves real-time searching on consumer GPUs (tested on RTX 2070S)
- Nearly doubles burst detection compared to Heimdall
- Classification accuracy exceeding 99% on FAST and GBT data
- Significantly reduces manual verification requirements

ðŸ“„ **Publication:** [DRAFTS: A Deep Learning-Based Radio Fast Transient Search Pipeline (arXiv:2410.03200)](https://arxiv.org/abs/2410.03200)

## New Features - SNR Analysis Integration

### ðŸ†• Signal-to-Noise Ratio (SNR) Analysis

The pipeline now includes comprehensive SNR analysis capabilities that enhance the detection and visualization of FRB candidates:

#### Key Features

- **Automatic SNR Calculation**: All candidate patches now display temporal profiles in SNR units (Ïƒ) instead of raw intensity
- **Robust Noise Estimation**: Uses Interquartile Range (IQR) method for noise estimation, robust to multiple pulses
- **Configurable Thresholds**: Set `SNR_THRESH` in `config.py` to highlight significant detections
- **Enhanced Visualizations**:
  - SNR profiles with peak annotations
  - Threshold lines for detection significance
  - Color-coded regions above threshold
  - Vertical markers showing peak positions

#### Configuration Parameters

Add these to your `config.py`:

```python
# SNR and Visualization Configuration
SNR_THRESH = 5.0  # Threshold for highlighting detections
SNR_OFF_REGIONS = [(-200, -100), (-50, 50), (100, 200)]  # Off-pulse regions
SNR_COLORMAP = "viridis"  # Colormap for waterfalls
SNR_HIGHLIGHT_COLOR = "red"  # Color for threshold highlighting
```

#### New Functions Available

- **`compute_snr_profile()`**: Calculate SNR profile from waterfall data
- **`find_snr_peak()`**: Locate and quantify peak SNR values
- **`inject_synthetic_frb()`**: Generate synthetic FRBs for testing
- **`estimate_sigma_iqr()`**: Robust noise estimation
- **`compute_detection_significance()`**: Statistical significance calculation

#### Enhanced Outputs

The pipeline now generates:

1. **SNR-enhanced patch plots** with annotated peaks and thresholds
2. **Composite summary plots** showing three SNR profiles:
   - Raw waterfall SNR (blue)
   - Dedispersed waterfall SNR (green)
   - Candidate patch SNR (orange)
3. **Peak markers** on all waterfall displays
4. **Significance annotations** with Ïƒ values

#### Testing

Run the SNR integration test:

```bash
python test_snr_integration.py
```

This creates test outputs in `test_snr_output/` to verify SNR functionality.

#### Scientific Benefits

- **Quantitative Assessment**: All detections now have quantitative SNR measurements
- **Improved Filtering**: Easy identification of high-significance candidates
- **Statistical Analysis**: Built-in significance calculation considering multiple trials
- **Robust Processing**: IQR-based noise estimation handles complex RFI environments

## Installation

Install all required dependencies from the `requirements.txt` file in the
repository root with:

```bash
pip install -r requirements.txt
```

## Usage

Training data and pre-trained models are available on HuggingFace:

- [DRAFTS-Data](https://huggingface.co/datasets/TorchLight/DRAFTS)
- [DRAFTS-Model](https://huggingface.co/TorchLight/DRAFTS)

### Pipeline Steps

1. **Preprocessing** â€“ Edit configuration variables such as `DM_range`,
   `block_size`, `data_path` and `save_path` in `d-center-main.py` or
   `d-resnet-main.py`. These scripts load FITS files and perform GPU-accelerated
   de-dispersion.
2. **Object detection** â€“ Run `d-center-main.py` with a trained CenterNet model.
   Detected candidates are saved as images with bounding boxes and optional
   `.npy` arrays for subsequent analysis.
3. **Binary classification** â€“ Execute `d-resnet-main.py` with the classification
   model path configured. The script outputs probability scores for each
   candidate and stores cropped bursts in the specified save directory.
4. **Visualization** â€“ When plotting waterfalls with `plot_waterfall_block`,
   set `normalize=True` to scale each frequency channel to unit mean and clip
   between the 5th and 95th percentiles for clear images across varying
   `SLICE_LEN` and DM ranges.

### Models

#### Object Detection

The object detection training code is in the `ObjectDet` folder.

1. Download data to `ObjectDet/Data`
2. Place `data_label.txt` in the same directory as `centernet_train.py`
3. Train using:

```bash
python centernet_train.py resnet18  # Use 'resnet50' for ResNet50 backbone
```

#### Binary Classification

The classification training code is in the `BinaryClass` folder.

1. Download data to `BinaryClass/Data`
2. Ensure data is organized in `True` and `False` subfolders
3. Train standard model:

```bash
python binary_train.py resnet18 BinaryNet  # Use 'resnet50' for ResNet50 backbone
```

4. Train with arbitrary image size support using `SpatialPyramidPool2D`:

```bash
python binary_train.py resnet18 SPPResNet
```

### Performance

To evaluate model performance:

1. Use the [FAST-FREX](https://doi.org/10.57760/sciencedb.15070) independent dataset
2. Place FITS files in `CheckRes/RawData/Data`
3. Place trained model checkpoints in the same directory as the Python files
4. Run evaluation scripts:
   - Files with `ddmt` for classification models
   - Files with `cent` for object detection models

**Dependencies:**

- Classification models depend on `binary_model.py`
- Object detection models depend on `centernet_utils.py` and `centernet_model.py`

## Search in Real Observation Data

For complete FAST observation data:

1. Refer to `d-center-main.py` and `d-resnet-main.py`
2. Modify the `data path` and `save path`
3. Set `FRB_TARGETS` in `effelsberg/config.py` to match the FRB names in your FITS files
4. Run the file

**Note:** The current search program automatically adapts to FAST and GBT observation data. For other telescopes, modify the `load_fits_file` function and related data reading functions.

## Contributing

Contributions to DRAFTS are welcome! Please feel free to submit issues or pull requests.

---

<div align="center">
  <sub>Searching for cosmic signals ðŸ”­âœ¨ðŸ“¡</sub>
</div># DRAFTS-FE
